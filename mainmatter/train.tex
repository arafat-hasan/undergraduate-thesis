\chapter[The Training]{The Training}
\markboth{Chap. 3\ \ \enspace Experimental methods}{Chap 2. Experimental methods}

\regularsection
\headerregularsection

\updatemylof % to be used with "list of figure divider per chapter" (see PREAMBLE)
\updatemylot % to be used with "list of table divider per chapter" (see PREAMBLE)

\begin{sloppypar} % to suppress overfull box
In brief the trainng process is:

\begin{itemize}
    \item First, pretrain the first 20 convolutional layers using the ImageNet 1000-class competition dataset, using a input size of 224x224
    \item Then, increase the input resolution to 448x448
    \item Train the full network for about 135 epochs using a batch size of 64, momentum of 0.9 and decay of 0.0005
    \item Learning rate schedule: for the first epochs, the learning rate was slowly raised from 0.001 to 0.01. Train for about 75 epochs and then start decreasing it.
    \item Use data augmentation with random scaling and translations, and randomly adjusting exposure and saturation.
\end{itemize}

\end{sloppypar}



\section{Training Settings}
Before modifying anything, first train with default settings to establish a performance baseline. A full list of train.py settings can be found in the train.py argparser.
\begin{itemize}
\item Epochs. Start with 300 epochs. If this overfits early then you can reduce epochs. If overfitting does not occur after 300 epochs, train longer, i.e. 600, 1200 etc epochs.
\item Image size. COCO trains at native resolution of \texttt{--img 640}, though due to the high amount of small objects in the dataset it can benefit from training at higher resolutions such as \texttt{--img 1280}. If there are many small objects then custom datasets will benefit from training at native or higher resolution. Best inference results are obtained at the same \texttt{--img} as the training was run at, i.e. if you train at \texttt{--img 1280} you should also test and detect at \texttt{--img 1280}.
\item Batch size. Use the largest \texttt{--batch-size} that your hardware allows for. Small batch sizes produce poor batchnorm statistics and should be avoided.
\item Hyperparameters. Default hyperparameters are in \texttt{hyp.scratch.yaml}. We recommend you train with default hyperparameters first before thinking of modifying any. In general, increasing augmentation hyperparameters will reduce and delay overfitting, allowing for longer trainings and higher final mAP. Reduction in loss component gain hyperparameters like \texttt{hyp['obj']} will help reduce overfitting in those specific loss components.
\end{itemize}




\section{Ensemble}
Ensemble modeling is a process where multiple diverse models are created to predict an outcome, either by using many different modeling algorithms or using different training data sets. The ensemble model then aggregates the prediction of each base model and results in once final prediction for the unseen data. The motivation for using ensemble models is to reduce the generalization error of the prediction. As long as the base models are diverse and independent, the prediction error of the model decreases when the ensemble approach is used. The approach seeks the wisdom of crowds in making a prediction. Even though the ensemble model has multiple base models within the model, it acts and performs as a single model. Most of the practical data mining solutions utilize ensemble modeling techniques. \cite{KOTU201517}





%=======================================================================
%%% References

% \clearpage
\phantomsection
\specialsection % put an indent, see preamble
\headerspecialsection

{\hypersetup{urlcolor=ntnu,linkcolor=sophia} % set clickable URL title color to black, not ntnu like in the main document

  \bibliographystyle{unsrtnat-mod}  % NATBIB ref style
  \bibliography{references}
}
