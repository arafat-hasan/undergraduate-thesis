@online{vehicleinoperation,
  author = {Statista},
  title = { Number of passenger cars and commercial vehicles in use worldwide from 2006 to 2015},
  year = {2021},
  url = {https://www.statista.com/statistics/281134/number-of-vehicles-in-use-worldwide},
  urldate = {2020-08-17}
}

@online{vehicleDef,
  author = {Merriam Webster},
  title = {Definition of vehicle},
  year = {2021},
  url = {https://www.merriam-webster.com/dictionary/vehicle},
  urldate = {2020-08-17}
}

@online{wikipediaVehicle,
  author = {Wikipedia},
  title = {Vehicle},
  year = {2021},
  url = {https://en.wikipedia.org/wiki/Vehicle},
  urldate = {2020-08-17}
}

@article{SriashikaAddala2020,
author = {Addala, Sriashika},
year = {2020},
month = {05},
pages = {},
title = {Research paper on vehicle detection and recognition},
doi = {10.13140/RG.2.2.34908.82561}
}



@article{SongH2019,
	abstract = {Intelligent vehicle detection and counting are becoming increasingly important in the field of highway management. However, due to the different sizes of vehicles, their detection remains a challenge that directly affects the accuracy of vehicle counts. To address this issue, this paper proposes a vision-based vehicle detection and counting system. A new high definition highway vehicle dataset with a total of 57,290 annotated instances in 11,129 images is published in this study. Compared with the existing public datasets, the proposed dataset contains annotated tiny objects in the image, which provides the complete data foundation for vehicle detection based on deep learning. In the proposed vehicle detection and counting system, the highway road surface in the image is first extracted and divided into a remote area and a proximal area by a newly proposed segmentation method; the method is crucial for improving vehicle detection. Then, the above two areas are placed into the YOLOv3 network to detect the type and location of the vehicle. Finally, the vehicle trajectories are obtained by the ORB algorithm, which can be used to judge the driving direction of the vehicle and obtain the number of different vehicles. Several highway surveillance videos based on different scenes are used to verify the proposed methods. The experimental results verify that using the proposed segmentation method can provide higher detection accuracy, especially for the detection of small vehicle objects. Moreover, the novel strategy described in this article performs notably well in judging driving direction and counting vehicles. This paper has general practical significance for the management and control of highway scenes.},
	author = {Song, Huansheng and Liang, Haoxiang and Li, Huaiyu and Dai, Zhe and Yun, Xu},
	da = {2019/12/30},
	date-added = {2021-10-02 22:14:10 +0600},
	date-modified = {2021-10-02 22:14:10 +0600},
	doi = {10.1186/s12544-019-0390-4},
	id = {Song2019},
	isbn = {},
	journal = {European Transport Research Review},
	number = {1},
	pages = {51},
	title = {Vision-based vehicle detection and counting system using deep learning in highway scenes},
	ty = {JOUR},
	url = {https://doi.org/10.1186/s12544-019-0390-4},
	volume = {11},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1186/s12544-019-0390-4}}



@article{Baran2016,
	abstract = {The paper presents a smart camera aimed at security and law enforcement applications for intelligent transportation systems. An extended background is presented first as a scholar literature review. The smart camera components and their capabilities for automatic detection and recognition of selected parameters of cars, as well as different aspects of the system efficiency, are described and discussed in detail in subsequent sections. Smart features of make and model recognition (MMR), license plate recognition (LPR) and color recognition (CR) are highlighted as the main benefits of the system. Their implementations, flowcharts and recognition rates are described, discussed and finally reported in detail. In addition to MMR, three different approaches, referred to as bag-of-features, scalable vocabulary tree and pyramid match, are also considered. The conclusion includes a discussion of the smart camera system efficiency as a whole, with an insight into potential future improvements.},
	author = {Baran, Remigiusz and Rusc, Tomasz and Fornalski, Pawe{\l}},
	da = {2016/09/01},
	date-added = {2021-10-02 22:16:20 +0600},
	date-modified = {2021-10-02 22:16:20 +0600},
	doi = {10.1007/s11042-015-3151-y},
	id = {Baran2016},
	isbn = {},
	journal = {Multimedia Tools and Applications},
	number = {17},
	pages = {10471--10493},
	title = {A smart camera for the surveillance of vehicles in intelligent transportation systems},
	ty = {JOUR},
	url = {https://doi.org/10.1007/s11042-015-3151-y},
	volume = {75},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1007/s11042-015-3151-y}}




@article{TangYong,
	abstract = {Vehicle detection and type recognition based on static images is highly practical and directly applicable for various operations in a traffic surveillance system. This paper will introduce the processing of automatic vehicle detection and recognition. First, Haar-like features and AdaBoost algorithms are applied for feature extracting and constructing classifiers, which are used to locate the vehicle over the input image. Then, the Gabor wavelet transform and a local binary pattern operator is used to extract multi-scale and multi-orientation vehicle features, according to the outside interference on the image and the random position of the vehicle. Finally, the image is divided into small regions, from which histograms sequences are extracted and concentrated to represent the vehicle features. Principal component analysis is adopted to reach a low dimensional histogram feature, which is used to measure the similarity of different vehicles in euler space and the nearest neighborhood is exploited for final classification. The typed experiment shows that our detection rate is over 97 {\%}, with a false rate of only 3 {\%}, and that the vehicle recognition rate is over 91 {\%}, while maintaining a fast processing time. This exhibits promising potential for implementation with real-world applications.},
	author = {Tang, Yong and Zhang, Congzhe and Gu, Renshu and Li, Peng and Yang, Bin},
	da = {2017/02/01},
	date-added = {2021-10-02 22:18:11 +0600},
	date-modified = {2021-10-02 22:18:11 +0600},
	doi = {10.1007/s11042-015-2520-x},
	id = {Tang2017},
	isbn = {},
	journal = {Multimedia Tools and Applications},
	number = {4},
	pages = {5817--5832},
	title = {Vehicle detection and recognition for intelligent traffic surveillance system},
	ty = {JOUR},
	url = {https://doi.org/10.1007/s11042-015-2520-x},
	volume = {76},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1007/s11042-015-2520-x}}





@article{ARINALDI2018259,
	abstract = {We present a traffic video analysis system based on computer vision techniques. The system is designed to automatically gather important statistics for policy makers and regulators in an automated fashion. These statistics include vehicle counting, vehicle type classification, estimation of vehicle speed from video and lane usage monitoring. The core of such system is the detection and classification of vehicles in traffic videos. We implement two models for this purpose, first is a MoG + SVM system and the second is based on Faster RCNN, a recently popular deep learning architecture for detection of objects in images. We show in our experiments that Faster RCNN outperforms MoG in detection of vehicles that are static, overlapping or in night time conditions. Faster RCNN also outperforms SVM for the task of classifying vehicle types based on appearances.},
	author = {Ahmad Arinaldi and Jaka Arya Pradana and Arlan Arventa Gurusinga},
	doi = {https://doi.org/10.1016/j.procs.2018.10.527},
	issn = {1877-0509},
	journal = {Procedia Computer Science},
	keywords = {Traffic Video Analysis, Vehicle Detection, Vehicle Classification, Faster RCNN},
	note = {INNS Conference on Big Data and Deep Learning},
	pages = {259-268},
	title = {Detection and classification of vehicles for traffic video analytics},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050918322361},
	volume = {144},
	year = {2018},
	Bdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/S1877050918322361},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.procs.2018.10.527}}

@inbook{lilijia,
author = {Jia, Lili and Wu, Dazhou and Mei, Lin and Zhao, Rui and Wang, Wenfei and Yu, Cai},
year = {2012},
month = {01},
pages = {592-599},
title = {Real-Time Vehicle Detection and Tracking System in Street Scenarios},
volume = {289},
isbn = {978-3-642-31967-9},
doi = {10.1007/978-3-642-31968-6_70}
}


@inproceedings{10.1007/978-3-642-31968-6_70,
	abstract = {The paper represents a framework for a vehicle detection, segmentation, and tracking system. The Challenge is to use a single monocular camera as input, in order to achieve a low cost final system that meets the requirements needed to undertake serial production in auto supervisory industry. We use a hierarchical method from the foreground region level to the vehicle level. The approach concerns stages of motion detection, edge detection, filtering, detection of the vehicle's position, and investigation into tracking cars by their appearance visual features. Color, which is one of the strongest cues, is used for the tracking step. The Continuously Adaptive MeanShift Algorithm (CamShift) is an adaptation of the Mean Shift algorithm, and we use it as a tracking method. Competitive performance results are provided using real video sequences in real traffic conditions.},
	address = {Berlin, Heidelberg},
	author = {Jia, Lili and Wu, Dazhou and Mei, Lin and Zhao, Rui and Wang, Wenfei and Yu, Cai},
	booktitle = {Communications and Information Processing},
	editor = {Zhao, Maotai and Sha, Junpin},
	isbn = {978-3-642-31968-6},
	pages = {592--599},
	publisher = {Springer Berlin Heidelberg},
	title = {Real-Time Vehicle Detection and Tracking System in Street Scenarios},
	year = {2012}}

@misc{redmon2016look,
      title={You Only Look Once: Unified, Real-Time Object Detection}, 
      author={Joseph Redmon and Santosh Divvala and Ross Girshick and Ali Farhadi},
      year={2016},
      eprint={1506.02640},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{orb,
author = {Rublee, Ethan and Rabaud, Vincent and Konolige, Kurt and Bradski, Gary},
year = {2011},
month = {11},
pages = {2564-2571},
title = {ORB: an efficient alternative to SIFT or SURF},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126544}
}

@article{10.1145/358669.358692,
author = {Fischler, Martin A. and Bolles, Robert C.},
title = {Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography},
year = {1981},
issue_date = {June 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/358669.358692},
doi = {10.1145/358669.358692},
abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental
data is introduced. RANSAC is capable of interpreting/smoothing data containing a
significant percentage of gross errors, and is thus ideally suited for applications
in automated image analysis where interpretation is based on the data provided by
error-prone feature detectors. A major portion of this paper describes the application
of RANSAC to the Location Determination Problem (LDP): Given an image depicting a
set of landmarks with known locations, determine that point in space from which the
image was obtained. In response to a RANSAC requirement, new results are derived on
the minimum number of landmarks needed to obtain a solution, and algorithms are presented
for computing these minimum-landmark solutions in closed form. These results provide
the basis for an automatic system that can solve the LDP under difficult viewing},
journal = {Commun. ACM},
month = jun,
pages = {381–395},
numpages = {15},
keywords = {camera calibration, location determination, image matching, automated cartography, model fitting, scene analysis}
}

@misc{wang2019cspnet,
      title={CSPNet: A New Backbone that can Enhance Learning Capability of CNN}, 
      author={Chien-Yao Wang and Hong-Yuan Mark Liao and I-Hau Yeh and Yueh-Hua Wu and Ping-Yang Chen and Jun-Wei Hsieh},
      year={2019},
      eprint={1911.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@inproceedings{10.1007/978-3-319-10578-9_23,
	abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224{\texttimes}224) input image. This requirement is ``artificial'' and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, ``spatial pyramid pooling'', to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101.},
	address = {Cham},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle = {Computer Vision -- ECCV 2014},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	isbn = {978-3-319-10578-9},
	pages = {346--361},
	publisher = {Springer International Publishing},
	title = {Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition},
	year = {2014}}
	
	
@misc{lin2015microsoft,
      title={Microsoft COCO: Common Objects in Context}, 
      author={Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Dollár},
      year={2015},
      eprint={1405.0312},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
